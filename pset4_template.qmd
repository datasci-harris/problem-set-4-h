---
title: "Problem Set 4"
author: "Sitong Guo, Helen Liu"
date: "Nov 3 2024"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Sitong Guo (rehinkerg)
    - Partner 2 (name and cnet ID): Hailun Liu (hailunl)
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\* SG \*\* \*\* HL \*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
PRVDR_CTGRY_SBTYP_CD(Provider Category Subtype Code), 
PRVDR_CTGRY_CD(Provider Category Code), 
FAC_NAME(Facility Name), 
PRVDR_NUM(CMS Certification Number), 
PGM_TRMNTN_CD(Termination Code), 
ZIP_CD(Address: ZIP Code).

2. 
    a.
    The file documented 7245 short term hospitals in 2016. This is likely to be 
    overestimated for the acting quantity since that there is fraction of non-operating 
    ones included. 
    ```{python}
    import pandas as pd
    
    pos2016 = pd.read_csv('E:/pos2016.csv',encoding='ISO-8859-1', dtype={'ZIP_CD': str})

    short_term_hospitals = pos2016[(pos2016['PRVDR_CTGRY_CD'] == 1) & (pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1)]

    num_hospitals = short_term_hospitals['PRVDR_NUM'].nunique()

    print(f"Number of short-term hospitals in 2016: {num_hospitals}")
    ```
    
    b.
    According to the KFF, as of Jul 07, 2016, there are nearly 5,000 short-term, acute care 
    hospitals in the United States. As stated by 2016 CMS Statistics, on the other hand, Medicare 
    short-term hospital was only 3436. This disvrepancy is due to that the file contains
    hospitals not acting and there might be other short-term hospitals besides acute care. 
    (The Number of U.S. Hospitals by Type (Total 5534), FY2016 by American Hospital Association 
    claimed that there were only 5534 hospitals in total in 2016, this is due to some 
    inexplicable divergences on statistical caliber on definition of hospital.) 

3. 

```{python}
import matplotlib.pyplot as plt
import altair as alt

pos2017 = pd.read_csv('E:/pos2017.csv', encoding='ISO-8859-1',dtype={'ZIP_CD': str})
pos2018 = pd.read_csv('E:/pos2018.csv', encoding='ISO-8859-1',dtype={'ZIP_CD': str})
pos2019 = pd.read_csv('E:/pos2019.csv', encoding='ISO-8859-1',dtype={'ZIP_CD': str})

def short_term(df):
    return df[(df['PRVDR_CTGRY_CD'] == 1) & (df['PRVDR_CTGRY_SBTYP_CD'] == 1)]

short_term_2016 = short_term(pos2016)
short_term_2017 = short_term(pos2017)
short_term_2018 = short_term(pos2018)
short_term_2019 = short_term(pos2019)

short_term_2016['Year'] = 2016
short_term_2017['Year'] = 2017
short_term_2018['Year'] = 2018
short_term_2019['Year'] = 2019

# append
all_years = pd.concat([short_term_2016, short_term_2017, short_term_2018, short_term_2019])

counts_by_year = all_years['Year'].value_counts().sort_index().reset_index()
counts_by_year.columns = ['Year', 'Count']

# set range for the y axis
y_min = counts_by_year['Count'].min() - 10
y_max = counts_by_year['Count'].max() + 10

alt.Chart(counts_by_year).mark_line(color='lightblue').encode(
    x=alt.X('Year:O', title='Year'),
    y=alt.Y('Count:Q', title='Number of Short-Term Hospitals', scale=alt.Scale(domain=[y_min, y_max]))
).properties(
    title ='Number of Short-Term Hospital Observations by Year', 
    height = 400,
    width = 400  
)
```

4. 
    a.
    
    ```{python}
    unique_hospitals_per_year = all_years.groupby('Year')['PRVDR_NUM'].nunique().reset_index()
    unique_hospitals_per_year.columns = ['Year', 'Unique_Count']

    y_min = unique_hospitals_per_year['Unique_Count'].min() - 10
    y_max = unique_hospitals_per_year['Unique_Count'].max() + 10

    alt.Chart(unique_hospitals_per_year).mark_line(color='coral').encode(
        x=alt.X('Year:O', title='Year'),
        y=alt.Y('Unique_Count:Q', title='Number of Unique Short-Term Hospitals', scale=alt.Scale(domain=[y_min, y_max]))
        ).properties(
        title ='Number of Unique Short-Term Hospitals per Year',
        width = 400,
        height = 400
    )
    ```
    
    b. 
    They are identical, which means that the short-term hospitals were each having a unique CCN 
    without redundancy, in the period of 2016-2019.

## Identify hospital closures in POS file (15 pts) (*)
1. 
```{python}
pos20161 = pos2016[pos2016['PRVDR_CTGRY_CD'] == 1]
pos20161 = pos20161[pos20161['PRVDR_CTGRY_SBTYP_CD'] == 1]
pos20171=pos2017[pos2017['PRVDR_CTGRY_CD'] == 1]
pos20171=pos20171[pos20171['PRVDR_CTGRY_SBTYP_CD'] == 1]
pos20181=pos2018[pos2018['PRVDR_CTGRY_CD'] == 1]
pos20181=pos20181[pos20181['PRVDR_CTGRY_SBTYP_CD'] == 1]
pos20191=pos2019[pos2019['PRVDR_CTGRY_CD'] == 1]
pos20191=pos20191[pos20191['PRVDR_CTGRY_SBTYP_CD'] == 1]

pos2016_subset = pos20161[['FAC_NAME', 'PRVDR_NUM', 'PGM_TRMNTN_CD', 'ZIP_CD']]
pos2017_subset = pos20171[['FAC_NAME', 'PRVDR_NUM', 'PGM_TRMNTN_CD', 'ZIP_CD']]
pos2018_subset = pos20181[['FAC_NAME', 'PRVDR_NUM', 'PGM_TRMNTN_CD', 'ZIP_CD']]
pos2019_subset = pos20191[['FAC_NAME', 'PRVDR_NUM', 'PGM_TRMNTN_CD', 'ZIP_CD']]

pos2016_subset = pos2016_subset.add_suffix('_2016').rename(columns={'PRVDR_NUM_2016': 'PRVDR_NUM'})
pos2017_subset = pos2017_subset.add_suffix('_2017').rename(columns={'PRVDR_NUM_2017': 'PRVDR_NUM'})
pos2018_subset = pos2018_subset.add_suffix('_2018').rename(columns={'PRVDR_NUM_2018': 'PRVDR_NUM'})
pos2019_subset = pos2019_subset.add_suffix('_2019').rename(columns={'PRVDR_NUM_2019': 'PRVDR_NUM'})

merged = pos2016_subset.merge(pos2017_subset, on='PRVDR_NUM', how='outer') \
                            .merge(pos2018_subset, on='PRVDR_NUM', how='outer') \
                            .merge(pos2019_subset, on='PRVDR_NUM', how='outer')

merged_2016_0 = merged[merged['PGM_TRMNTN_CD_2016'] == 0]

suspected = merged_2016_0[
    ~((merged_2016_0['PGM_TRMNTN_CD_2016'] == 0) & 
      (merged_2016_0['PGM_TRMNTN_CD_2017'] == 0) & 
      (merged_2016_0['PGM_TRMNTN_CD_2018'] == 0) & 
      (merged_2016_0['PGM_TRMNTN_CD_2019'] == 0))
]
suspected = suspected[['PRVDR_NUM', 'FAC_NAME_2016', 'ZIP_CD_2016', 'PGM_TRMNTN_CD_2016', 'PGM_TRMNTN_CD_2017', 'PGM_TRMNTN_CD_2018', 'PGM_TRMNTN_CD_2019']]

count = suspected['FAC_NAME_2016'].unique()
print(len(count))
```
There are 174 hospital that fit this definition( hospitals that were active in 2016 that were suspected to have closed by 2019)

2. 
```{python}
import numpy as np
conditions = [
    (suspected['PGM_TRMNTN_CD_2017'] != 0) & (suspected['PGM_TRMNTN_CD_2018'] != 0) & (suspected['PGM_TRMNTN_CD_2019'] != 0),
    (suspected['PGM_TRMNTN_CD_2017'] == 0) & (suspected['PGM_TRMNTN_CD_2018'] != 0) & (suspected['PGM_TRMNTN_CD_2019'] != 0),
    (suspected['PGM_TRMNTN_CD_2017'] == 0) & (suspected['PGM_TRMNTN_CD_2018'] == 0) & (suspected['PGM_TRMNTN_CD_2019'] != 0)
]
choices = [2017, 2018, 2019]
suspected['YearOfSuspectedClosure'] = np.select(conditions, choices, default=np.nan)

sorted_suspected = suspected.sort_values(by='FAC_NAME_2016')[['FAC_NAME_2016', 'YearOfSuspectedClosure','ZIP_CD_2016']]

top_10_hospitals = sorted_suspected.head(10)
print(top_10_hospitals)
```

3. 
    a.
    ```{python}
    #hospitals active each year
    active_2016 = pos2016_subset[pos2016_subset['PGM_TRMNTN_CD_2016'] == 0][['ZIP_CD_2016', 'PRVDR_NUM']]
    active_2017 = pos2017_subset[pos2017_subset['PGM_TRMNTN_CD_2017'] == 0][['ZIP_CD_2017', 'PRVDR_NUM']]
    active_2018 = pos2018_subset[pos2018_subset['PGM_TRMNTN_CD_2018'] == 0][['ZIP_CD_2018', 'PRVDR_NUM']]
    active_2019 = pos2019_subset[pos2019_subset['PGM_TRMNTN_CD_2019'] == 0][['ZIP_CD_2019', 'PRVDR_NUM']]

    #by ZIP each year
    active_count_2016 = active_2016.groupby('ZIP_CD_2016').size().reset_index(name='ActiveCount_2016')
    active_count_2017 = active_2017.groupby('ZIP_CD_2017').size().reset_index(name='ActiveCount_2017')
    active_count_2018 = active_2018.groupby('ZIP_CD_2018').size().reset_index(name='ActiveCount_2018')
    active_count_2019 = active_2019.groupby('ZIP_CD_2019').size().reset_index(name='ActiveCount_2019')

    active_count_2016 = active_count_2016.rename(columns={'ZIP_CD_2016': 'ZIP_CD'})
    active_count_2017 = active_count_2017.rename(columns={'ZIP_CD_2017': 'ZIP_CD'})
    active_count_2018 = active_count_2018.rename(columns={'ZIP_CD_2018': 'ZIP_CD'})
    active_count_2019 = active_count_2019.rename(columns={'ZIP_CD_2019': 'ZIP_CD'})

    # merge active counts to compare by ZIP
    zip_counts = active_count_2016.merge(active_count_2017, on='ZIP_CD', how='outer') \
                                .merge(active_count_2018, on='ZIP_CD', how='outer') \
                                .merge(active_count_2019, on='ZIP_CD', how='outer')

    suspected = suspected.merge(zip_counts, left_on='ZIP_CD_2016', right_on='ZIP_CD', how='left')

    #number of active did not decrease...questionable!
    suspected['IsMerger'] = np.where(
        ((suspected['YearOfSuspectedClosure'] == 2017) & (suspected['ActiveCount_2017'] >= suspected['ActiveCount_2016'])) |
        ((suspected['YearOfSuspectedClosure'] == 2018) & (suspected['ActiveCount_2018'] >= suspected['ActiveCount_2017'])) |
        ((suspected['YearOfSuspectedClosure'] == 2019) & (suspected['ActiveCount_2019'] >= suspected['ActiveCount_2018'])), True, False)

    #potential merger/acquisition
    corrected_closures = suspected[~suspected['IsMerger']]
    merger_count = suspected['IsMerger'].sum()
    print(f"potential mergers/acquisitions: {merger_count}")
    ```
   
    b.
    ```{python}
    #corrected closures
    corrected_closure_count = corrected_closures['FAC_NAME_2016'].nunique()
    print(f"hospitals correcting for m/a: {corrected_closure_count}")
    ```

    c.
    ```{python}
    sorted_corrected_closures = corrected_closures.sort_values(by='FAC_NAME_2016')[['FAC_NAME_2016', 'YearOfSuspectedClosure', 'ZIP_CD_2016']]
    print(sorted_corrected_closures.head(10))
    ```

## Download Census zip code shapefile (10 pt) 
1. 
    a.
    (1).shp (Shape file): Main file that has feature geometrics, such as points, lines, or polygons that represent the shapes of geographic features inclluding ZIP code boundaries.
    
    (2).shx (Shape index file) : Contains an positional index of the geometries in the shp file, accelerating access to geographic features.
    
    (3).dbf (database file): a tabular file with attribute information, in dBASE format that stores attributes or additional data about each shape in the shp file.
    
    (4).prj (projection file): Describes the Coordinate Reference System (CRS). Contains information about the system and projection used in the shp, ensuring that the data aligns correctly with other geographic data.

    (5).xml: Detailed text information about the dataset like source, description, date, attribute definitions, and other information for understanding the data's context and structure.

    b. 
    ```{python}
    import os
    directory = 'E:/SeriousBusiness/Applications/uchicago/python2'

    for filename in os.listdir(directory):
        file_path = os.path.join(directory, filename)
        if os.path.isfile(file_path):
            file_size = os.path.getsize(file_path) 
            print(f"{filename}: {file_size} KB")
    ```

2. 
```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

#number of hospitals per ZIP
hospitals_per_zip = active_2016.groupby('ZIP_CD_2016').size().reset_index(name='Hospital_Count')
hospitals_per_zip.columns = ['ZIP_CD', 'Hospital_Count']
```

```{python}
zip_codes = gpd.read_file('E:/SeriousBusiness/Applications/uchicago/python2/gz_2010_us_860_00_500k.shp')
```
```{python}
#print(zip_codes.columns)
# Index(['GEO_ID', 'ZCTA5', 'NAME', 'LSAD', 'CENSUSAREA', 'geometry'], dtype='object')
#Texas
texas_zip_codes = zip_codes[zip_codes['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]
```

```{python}
texas_zip_hospitals = texas_zip_codes.merge(hospitals_per_zip, left_on='ZCTA5', right_on='ZIP_CD', how='left')
#fill missing hospital counts with 0!
texas_zip_hospitals['Hospital_Count'] = texas_zip_hospitals['Hospital_Count'].fillna(0)
texas_zip_hospitals.head(5)
```

```{python}
plt.figure(figsize=(18, 15))
texas_zip_hospitals.plot(column='Hospital_Count', cmap='Blues', linewidth=0.8, edgecolor='0.8', legend=True)
plt.title('Number of Hospitals by ZIP in Texas(2016)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)
1. 
```{python}
shp = gpd.read_file('E:/SeriousBusiness/Applications/uchicago/python2/gz_2010_us_860_00_500k.shp')
shx = gpd.read_file('E:/SeriousBusiness/Applications/uchicago/python2/gz_2010_us_860_00_500k.shx')
zips_all_centroids = shp.copy()
zips_all_centroids['geometry'] = shp.geometry.centroid
```

Dimensions:
The dimensions output from the GeoDataFramel created for the centroid of each zip code nationally gives the number of rows and columns which is equal
to the number of unique ZIP codes in the data.
Each row represents a unique ZIP Code area, and the columns represent the .
The coordinate position of the centroids(each ZIP Code).

Columns meaning:
1.GEO_ID: A unique identifier for each geographic region, helping to distinguish each ZIP Code area.
2.ZCTA5:This is the 5-digit ZIP Code Tabulation Area (ZCTA), a representation of ZIP Code regions used in census data.
3.NAME: Typically represents the ZIP Code for the area.
4.LSAD: Legal/Statistical Area Description, identifying the type of area, such as city or rural.
5.geometry: Contains the centroid point of each ZIP Code area.This is a Point geometry representing the geometric center of each ZIP Code area.

2. 
In subsets of zips_texas_centroids,there are 1935 unique zip codes.
In subsets of zips_texas_borderstates_centroids,there are 4057 unique zip codes.
```{python}
import geopandas as gpd
zips_all_centroids['ZIP_INT'] = zips_all_centroids['ZCTA5'].astype(int)
texas_condition = (
    (zips_all_centroids['ZIP_INT'] >= 75000) & (zips_all_centroids['ZIP_INT'] <= 79999)
)

border_states_condition = (
    ((zips_all_centroids['ZIP_INT'] >= 87000) & (zips_all_centroids['ZIP_INT'] <= 88499)) |  
    ((zips_all_centroids['ZIP_INT'] >= 70000) & (zips_all_centroids['ZIP_INT'] <= 72999)) |  
    ((zips_all_centroids['ZIP_INT'] >= 73000) & (zips_all_centroids['ZIP_INT'] <= 79999))   
)
zips_texas_centroids = zips_all_centroids[texas_condition]
zips_texas_borderstates_centroids = zips_all_centroids[texas_condition | border_states_condition]
unique_texas_zips = zips_texas_centroids['ZCTA5'].nunique()
unique_texas_border_zips = zips_texas_borderstates_centroids['ZCTA5'].nunique()
print("Unique ZIP Codes in Texas:", unique_texas_zips)
print("Unique ZIP Codes in Texas and bordering states:", unique_texas_border_zips)
```

3. 
There are 468 rows in zips_withhospital_centroids .

We’ll use an inner join since we only want the zip codes that appear in both datasets (those in zips_texas_borderstates_centroids that are also in merged_gdf).The merge will be based on the zip code column, typically named something like 'ZIP_CD_2016' in both GeoDataFrames. Ensure both columns are named consistently for the merge.
```{python}
dbf = gpd.read_file('E:/SeriousBusiness/Applications/uchicago/python2/gz_2010_us_860_00_500k.dbf')
merged_2016_0['ZIP_INT'] = pd.to_numeric(merged_2016_0['ZIP_CD_2016'], errors='coerce')
border_states_condition = (
    ((merged_2016_0['ZIP_INT'] >= 87000) & (merged_2016_0['ZIP_INT'] <= 88499)) | 
    ((merged_2016_0['ZIP_INT'] >= 70000) & (merged_2016_0['ZIP_INT'] <= 72999)) |  
    ((merged_2016_0['ZIP_INT'] >= 73000) & (merged_2016_0['ZIP_INT'] <= 79999))    
)
border_states_hopital_2016 = merged_2016_0[border_states_condition]
border_states_hopital_2016 = border_states_hopital_2016.drop(columns=['ZIP_INT'])

texas_borderstates_hopital = border_states_hopital_2016.groupby('ZIP_CD_2016').size().reset_index(name='COUNT')
texas_borderstates_hopital['ZIP_CD_2016'] = texas_borderstates_hopital['ZIP_CD_2016'].astype(int)
texas_borderstates_hopital['ZIP_CD_2016'] = texas_borderstates_hopital['ZIP_CD_2016'].astype(str)

df = pd.DataFrame(zips_texas_borderstates_centroids.drop(columns='geometry'))
df = df.rename(columns={'ZCTA5': 'ZIP_CD_2016'})
df['ZIP_CD_2016'].astype(str)
```
```{python}
merged_gdf = df.merge(
    texas_borderstates_hopital, 
    on='ZIP_CD_2016',  
    how='right' 
)
print(f"There are {len(merged_gdf)} rows in zips_withhospital_centroids .")
```
```{python}
zips_withhospital_centroids = gpd.GeoDataFrame(merged_gdf)
```

4. 
    a.It will take about 0.0099 seconds for subset to 10 zip codes.And the whole process will take about 1.93 seconds.
```{python}
import time
#shp = gpd.read_file('E:/SeriousBusiness/Applications/uchicago/python2/gz_2010_us_860_00_500k.shp')
zips_texas_centroids = shp[shp['ZCTA5'].astype(str).str[:2].isin(['75', '76', '77', '78', '79'])]
zips_withhospital_centroids = shp[shp['ZCTA5'].isin(merged_gdf['ZIP_CD_2016'])]

zips_texas_centroids['geometry'] = zips_texas_centroids.geometry.centroid
zips_withhospital_centroids['geometry'] = zips_withhospital_centroids.geometry.centroid

zips_texas_subset = zips_texas_centroids.head(10)
start_time = time.time()
subset_join_result = gpd.sjoin_nearest(
    zips_texas_subset,
    zips_withhospital_centroids,
    how="inner",
    distance_col="distance"
)
time_taken = time.time() - start_time
subset_join_result, time_taken
```

    b.As for the full calculation,it takes about 0.07 seconds,which is faster than l estimated.
```{python}
zips_texas_centroids = zips_texas_centroids.to_crs(epsg=3857)
zips_withhospital_centroids = zips_withhospital_centroids.to_crs(epsg=3857)
start_time = time.time()
subset_join_result = gpd.sjoin_nearest(
    zips_texas_centroids,
    zips_withhospital_centroids,
    how="inner",
    distance_col="distance"
)
time_taken = time.time() - start_time
subset_join_result, time_taken
```

    c.The .prj file specifies that the unit is in "Degree" (angular unit)
In this context, "Degree" represents degrees of latitude and longitude.
On the Earth's surface, 1 degree of latitude is approximately equal to 69 miles.
The distance represented by 1 degree of longitude varies by latitude, but in mid-latitude regions like the United States, 1 degree of longitude is roughly 53 miles.
So, the approximate conversions are:
1 degree of latitude ≈ 69 miles
1 degree of longitude ≈ 53 miles (suitable for mid-latitude areas in the U.S.)
```{python}
from pyproj import CRS
prj_file = "E:/SeriousBusiness/Applications/uchicago/python2/gz_2010_us_860_00_500k.prj"
with open(prj_file, 'r') as file:
    prj_text = file.read()
crs = CRS.from_wkt(prj_text)
print(crs)
```

5. 
    a.Unit is inclueded distance.
```{python}
subset_join_result = gpd.sjoin_nearest(
    zips_texas_centroids,
    zips_withhospital_centroids,
    how="inner",
    distance_col="distance"
)
```

    b.the average distance is 0.21101748566398393.
the average distance in miles is 14.49miles
```{python}
dd = subset_join_result.drop(columns='geometry')
mean_distance = dd['distance'].mean()
print(mean_distance)
```

    c.
```{python}
import matplotlib.pyplot as plt
subset_join_result.plot(column = 'distance').set_axis_off()
plt.axis("off")
```

## Effects of closures on access in Texas (15 pts)

1. 
```{python}
texas_closures = sorted_corrected_closures[sorted_corrected_closures['ZIP_CD_2016'].str.startswith(('75', '76', '77', '78', '79'))]
closures_by_zip = texas_closures.groupby('ZIP_CD_2016').size().reset_index(name='Number_of_Closures')
closures_by_zip.columns = ['ZIP Code', 'Number of Closures']
print(closures_by_zip)
```

2. 
```{python}
texas_zip_closures = texas_zip_codes.merge(closures_by_zip, left_on='ZCTA5', right_on='ZIP Code', how='left')
#Fill nan with 0..
texas_zip_closures['Number of Closures'] = texas_zip_closures['Number of Closures'].fillna(0)

affected_zip_count = texas_zip_closures[texas_zip_closures['Number of Closures'] > 0]['ZCTA5'].nunique()
print(f"Directly affected ZIPs in Texas: {affected_zip_count}")

plt.figure(figsize=(12, 10))
texas_zip_closures.plot(column='Number of Closures', cmap='OrRd', linewidth=0.8, edgecolor='0.8', legend=True)
plt.title('Texas ZIP Affected by Hospital Closure(2016-2019)')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.show()
```

3. 
```{python}
# 16093.44 meters
directly_affected_zips = texas_zip_closures[texas_zip_closures['Number of Closures'] > 0]
'''
 IMPORTANT!: print(directly_affected_zips.crs) to see the CRS! IT IS NOT METER.
'''
directly_affected_zips = directly_affected_zips.to_crs("EPSG:3083")
texas_zip_codes = texas_zip_codes.to_crs("EPSG:3083")
# seems like this question doesn't need buffer around centroid.
'''
directly_affected_zips['centroid'] = directly_affected_zips.geometry.centroid
directly_affected_zips['buffer'] = directly_affected_zips['centroid'].buffer(16093.4)  
directly_affected_zips.set_geometry('buffer', inplace=True)
'''
directly_affected_zips['geometry'] = directly_affected_zips.geometry.buffer(16093.4)
affected_zips = gpd.sjoin(texas_zip_codes, directly_affected_zips, how='inner', predicate='intersects')

# sift out those directly; Must use ZCTA5_left, since the zcta5_right and ZIP Code is introduced from directly_affected table by the joins intersect, thus don't comprise those in the buffer!
indirectly_affected_zips = affected_zips[~affected_zips['ZCTA5_left'].isin(directly_affected_zips['ZCTA5'])]
indirect_zip_count = indirectly_affected_zips['ZCTA5_left'].nunique()
print("Number of indirectly affected ZIP codes:", indirect_zip_count)
```
```{python}
print(affected_zips.columns)
```
```{python}
# buffered areas
fig, ax = plt.subplots(figsize=(10, 10))
texas_zip_codes.plot(ax=ax, color='lightgrey', edgecolor='black')
directly_affected_zips.plot(ax=ax, color='red', alpha=0.5)
plt.title("10-Mile Buffers Around Directly Affected ZIP Codes")
plt.show()
```

4. 
```{python}
def categorize_zip(zip):
    if zip['ZCTA5'] in directly_affected_zips['ZIP Code'].values:
        return 'Directly Affected'
    elif zip['ZCTA5'] in indirectly_affected_zips['ZCTA5_left'].values:
        return 'Indirectly Affected'
    else:
        return 'Not Affected'
# apply function onto dataframe, use .apply(func, axis=1): 
texas_zip_codes['Category'] = texas_zip_codes.apply(categorize_zip, axis=1)
color_mapping = {
    'Directly Affected': 'red',
    'Indirectly Affected': 'gold',
    'Not Affected': 'lightgray'
}
texas_zip_codes['Color'] = texas_zip_codes['Category'].map(color_mapping)

fig, ax = plt.subplots(1, 1, figsize=(10, 10))
texas_zip_codes.plot(ax=ax, color=texas_zip_codes['Color'], edgecolor='gray',legend = True) #simply adding 'legend = true' when setting colors based on a non-numeric, manually defined Color column cannot work. 
ax.set_title('Texas ZIP Codes Affected by Closures')

#legend with custom patches
import matplotlib.patches as mpatches
legend_patches = [mpatches.Patch(color=color, label=label) for label, color in color_mapping.items()]
ax.legend(handles=legend_patches, title="Category")
plt.show()
```

## Reflecting on the exercise (10 pts) 
1.This is indeed doubtable. We are given the knowledge that when a TorF closure takes place, that will be reflected in the year's file. Thus, for an actual closure, 
the number of active hospital next year shall remain unaltered, and for false closure(merger/acquisition), the number next year should rise by 1. This indicates that the 
"non-decrease" criteria will fail to winnow out the false closures, they are all non-decrease! In a nutshell, the number we calculated in section2 is still the 
aggregate number of those 'simply non-active shown by termination code'. 
A remedy could be changing from 'not decrease' to 'increase', or sifting out those with more than 1 unique CMS number in the four years.

2.When we are identifying zip codes affected by closures,it may have duplicated date if the hopital is merged accross difference zip codes. For the hopital which located near the froniter of a zipcode may
happens that sitation. I think the best way to improve this sitation is by identifying the merge hopital with a 'merger_number',which may help us better identify the true closure.Instead of calculating distance 
alone, measure average travel time to the nearest hospital, considering public transportation, road quality, and traffic patterns. This would give a more realistic view of accessibility.Adjust the measure 
to account for population density and demographics. High-population or high-need areas could be weighted more heavily, reflecting a higher demand for healthcare services.